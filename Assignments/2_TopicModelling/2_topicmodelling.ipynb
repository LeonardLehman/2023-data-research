{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KV-UrkvK3HBQ"
   },
   "source": [
    "# Data-Driven Research Assignment 2: Topic Modeling\n",
    "This notebook contains the second, collaborative, graded assignment of the 2023 Data-Driven Research course. In this assignment you'll use a topic modeling tool in order to uncover the ''topics'' of a large set of reviews of popular films. \n",
    "\n",
    "To complete the assignment, complete **Part 1, Part 2, Part 3 and Part 4** of the **Your Model** section at the end."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "13WY-Pd4m1Nq"
   },
   "source": [
    "This is a collaborative assignment. In the text cell below, please include all the names of your group members.\n",
    "\n",
    "If you used code or a solution from the internet (such as StackOverflow) or another external resource, please make reference to it (in any format). Unattributed copied code will be considered plagiarism and therefore fraud.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "FnYtJb9olXd0"
   },
   "source": [
    "**Authors of this answer:** Leonards Leimanis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gV4eOyrVqRHQ"
   },
   "source": [
    "# 1. Introduction\n",
    "\n",
    "You'll use a Topic Modelling tool from Gensim, a popular library for topic modelling in Python, though these days mainly known for its implementation of Word2Vec to train word embeddings (dense representations). Using this library, you will model topics based on reviews of popular films. The reviews are stored in plain text files, organized by film and rating. The aim of this exercise is to familiarize you with the topic modeling process and its output and to get insight in what kinds of topics are modeled."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uaPXV7qlq9oa"
   },
   "source": [
    "# 2. Preparation\n",
    "\n",
    "This assignment comes with the following files:\n",
    "\n",
    "\n",
    "1.   The reviews of the films. This is the data in which we want to find topics. They are found in the movie2k/txt_sentoken directory. There are then two types: negative reviews (neg directory) and positive reviews (pos directory). The reviews are already tokenized.\n",
    "2.   Stopword list files. They are found in the stopwords directory.\n",
    "\n",
    "Let's start by loading the movie reviews from the files (I'll do it for you):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "jp-zI1sNQfPD"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded reviews from movie2k/txt_sentoken/pos containing 787051 tokens in total.\n",
      "Loaded reviews from movie2k/txt_sentoken/neg containing 705630 tokens in total.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def load_reviews(folder_path):\n",
    "    reviews = [] #Make a list to put the reviews in\n",
    "    reviewnames = [] # Make a list to put the review filenames in (to be able to look them up later)\n",
    "    tokens = 0 #Make a counter for the number of tokens\n",
    "    \n",
    "    for file in os.listdir(folder_path):\n",
    "        #Loop through all the text files in the folder, each containing one review\n",
    "        \n",
    "        if not file.endswith('.txt'):  #Only read text files\n",
    "            continue\n",
    "\n",
    "        file_path = os.path.join(folder_path, file)\n",
    "\n",
    "        #Open the text file and read its contents\n",
    "        with open(file_path, encoding='utf-8') as infile:\n",
    "            review = infile.read()\n",
    "        reviewnames.append(file)\n",
    "            \n",
    "        # Turn the string with the review into a list of words (this is easy because it is already tokenized)\n",
    "        review = review.split()\n",
    "        # And add it to the list\n",
    "        reviews.append(review)\n",
    "        # To count the number of tokens processed so far\n",
    "        tokens = tokens + len(review)\n",
    "\n",
    "    print(f\"Loaded reviews from {folder_path} containing {tokens} tokens in total.\") \n",
    "    return reviews, reviewnames\n",
    "        \n",
    "folder_path = \"movie2k/txt_sentoken\"\n",
    "    \n",
    "movie_reviews_pos, movie_reviewnames_pos = load_reviews(folder_path + \"/pos\") #Load the positive reviews\n",
    "movie_reviews_neg, movie_reviewnames_neg = load_reviews(folder_path + \"/neg\") #Load the negative reviews\n",
    "\n",
    "movie_reviews = movie_reviews_pos + movie_reviews_neg #Combine the lists of positive and negative reviews into one\n",
    "movie_reviewnames = movie_reviewnames_pos + movie_reviewnames_neg #The same for the list of filenames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tUb1NfU7Q_do"
   },
   "source": [
    "If you are working on Google Colab, you will probably have to change the path to the files to something that Google Colab has access to. For example, you could put the files on your Google Drive and then load them from there, as we did in Coding the Humanities. For more details about how to work with files in Python and load them from Google Drive, have a look at the Coding the Humanities course notebook on Files: https://github.com/bloemj/2023-coding-the-humanities/blob/main/notebooks/4_ReadingAndWritingFiles.ipynb\n",
    "\n",
    "How to load files off Google Drive is explained at the beginning there."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "Now that we have loaded the text, you might want to perform some pre-processing steps to be able to create a better bag-of-words model in which all forms of a word are mapped to a single number. For example, you could remove the punctuation characters, or you could perform lemmatization or stemming, which we discussed in the lecture. This would be the place to do it by writing a preprocessing function that accepts a list of movie reviews as its argument and returns a preprocessed list of movie reviews. Feel free to use your knowledge of text normalization from Coding the Humanities or the functions you wrote then. Here is some information on how to perform stemming with NLTK: https://www.nltk.org/howto/stem.html\n",
    "\n",
    "You can also try other forms of preprocessing, if you are able to do it.\n",
    "\n",
    "Make sure to also keep the unmodified reviews, so you can compare the results with preprocessing and without preprocessing.\n",
    "\n",
    "**Part 1: Preprocessing**\n",
    "\n",
    "You can also skip this part for now - it is not required to perform the topic modelling, but you will get better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import *\n",
    "import string\n",
    "\n",
    "preprocessed_movie_reviews = []\n",
    "\n",
    "removable_characters = [',', '(', ')', '.', '?', '!', ':', '~', '`', ';', '\"', \"'\", 'Â»', \"<p>\", \"</p>\", \"<P>\", \"</P>\"]\n",
    "\n",
    "for review in movie_reviews:\n",
    "    normalized = []\n",
    "    for token in review:\n",
    "        if token in string.punctuation:\n",
    "            continue\n",
    "        else:\n",
    "            normalized.append(token)\n",
    "    preprocessed_movie_reviews.append(normalized)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['films', 'adapted', 'from', 'comic', 'books', 'have', 'had', 'plenty', 'of', 'success', ',', 'whether', \"they're\", 'about', 'superheroes', '(', 'batman', ',', 'superman', ',', 'spawn', ')', ',', 'or', 'geared', 'toward', 'kids', '(', 'casper', ')', 'or', 'the', 'arthouse', 'crowd', '(', 'ghost', 'world', ')', ',', 'but', \"there's\", 'never', 'really', 'been', 'a', 'comic', 'book', 'like', 'from', 'hell', 'before', '.', 'for', 'starters', ',', 'it', 'was', 'created', 'by', 'alan', 'moore', '(', 'and', 'eddie', 'campbell', ')', ',', 'who', 'brought', 'the', 'medium', 'to', 'a', 'whole', 'new', 'level', 'in', 'the', 'mid', \"'80s\", 'with', 'a', '12-part', 'series', 'called', 'the', 'watchmen', '.', 'to', 'say', 'moore', 'and', 'campbell', 'thoroughly', 'researched', 'the', 'subject', 'of', 'jack', 'the', 'ripper', 'would', 'be', 'like', 'saying', 'michael', 'jackson', 'is', 'starting', 'to', 'look', 'a', 'little', 'odd', '.', 'the', 'book', '(', 'or', '\"', 'graphic', 'novel', ',', '\"', 'if', 'you', 'will', ')', 'is', 'over', '500', 'pages', 'long', 'and', 'includes', 'nearly', '30', 'more', 'that', 'consist', 'of', 'nothing', 'but', 'footnotes', '.', 'in', 'other', 'words', ',', \"don't\", 'dismiss', 'this', 'film', 'because', 'of', 'its', 'source', '.', 'if', 'you', 'can', 'get', 'past', 'the', 'whole', 'comic', 'book', 'thing', ',', 'you', 'might', 'find', 'another', 'stumbling', 'block', 'in', 'from', \"hell's\", 'directors', ',', 'albert', 'and', 'allen', 'hughes', '.', 'getting', 'the', 'hughes', 'brothers', 'to', 'direct', 'this', 'seems', 'almost', 'as', 'ludicrous', 'as', 'casting', 'carrot', 'top', 'in', ',', 'well', ',', 'anything', ',', 'but', 'riddle', 'me', 'this', ':', 'who', 'better', 'to', 'direct', 'a', 'film', \"that's\", 'set', 'in', 'the', 'ghetto', 'and', 'features', 'really', 'violent', 'street', 'crime', 'than', 'the', 'mad', 'geniuses', 'behind', 'menace', 'ii', 'society', '?', 'the', 'ghetto', 'in', 'question', 'is', ',', 'of', 'course', ',', 'whitechapel', 'in', '1888', \"london's\", 'east', 'end', '.', \"it's\", 'a', 'filthy', ',', 'sooty', 'place', 'where', 'the', 'whores', '(', 'called', '\"', 'unfortunates', '\"', ')', 'are', 'starting', 'to', 'get', 'a', 'little', 'nervous', 'about', 'this', 'mysterious', 'psychopath', 'who', 'has', 'been', 'carving', 'through', 'their', 'profession', 'with', 'surgical', 'precision', '.', 'when', 'the', 'first', 'stiff', 'turns', 'up', ',', 'copper', 'peter', 'godley', '(', 'robbie', 'coltrane', ',', 'the', 'world', 'is', 'not', 'enough', ')', 'calls', 'in', 'inspector', 'frederick', 'abberline', '(', 'johnny', 'depp', ',', 'blow', ')', 'to', 'crack', 'the', 'case', '.', 'abberline', ',', 'a', 'widower', ',', 'has', 'prophetic', 'dreams', 'he', 'unsuccessfully', 'tries', 'to', 'quell', 'with', 'copious', 'amounts', 'of', 'absinthe', 'and', 'opium', '.', 'upon', 'arriving', 'in', 'whitechapel', ',', 'he', 'befriends', 'an', 'unfortunate', 'named', 'mary', 'kelly', '(', 'heather', 'graham', ',', 'say', 'it', \"isn't\", 'so', ')', 'and', 'proceeds', 'to', 'investigate', 'the', 'horribly', 'gruesome', 'crimes', 'that', 'even', 'the', 'police', 'surgeon', \"can't\", 'stomach', '.', 'i', \"don't\", 'think', 'anyone', 'needs', 'to', 'be', 'briefed', 'on', 'jack', 'the', 'ripper', ',', 'so', 'i', \"won't\", 'go', 'into', 'the', 'particulars', 'here', ',', 'other', 'than', 'to', 'say', 'moore', 'and', 'campbell', 'have', 'a', 'unique', 'and', 'interesting', 'theory', 'about', 'both', 'the', 'identity', 'of', 'the', 'killer', 'and', 'the', 'reasons', 'he', 'chooses', 'to', 'slay', '.', 'in', 'the', 'comic', ',', 'they', \"don't\", 'bother', 'cloaking', 'the', 'identity', 'of', 'the', 'ripper', ',', 'but', 'screenwriters', 'terry', 'hayes', '(', 'vertical', 'limit', ')', 'and', 'rafael', 'yglesias', '(', 'les', 'mis', '?', 'rables', ')', 'do', 'a', 'good', 'job', 'of', 'keeping', 'him', 'hidden', 'from', 'viewers', 'until', 'the', 'very', 'end', '.', \"it's\", 'funny', 'to', 'watch', 'the', 'locals', 'blindly', 'point', 'the', 'finger', 'of', 'blame', 'at', 'jews', 'and', 'indians', 'because', ',', 'after', 'all', ',', 'an', 'englishman', 'could', 'never', 'be', 'capable', 'of', 'committing', 'such', 'ghastly', 'acts', '.', 'and', 'from', \"hell's\", 'ending', 'had', 'me', 'whistling', 'the', 'stonecutters', 'song', 'from', 'the', 'simpsons', 'for', 'days', '(', '\"', 'who', 'holds', 'back', 'the', 'electric', 'car/who', 'made', 'steve', 'guttenberg', 'a', 'star', '?', '\"', ')', '.', \"don't\", 'worry', '-', \"it'll\", 'all', 'make', 'sense', 'when', 'you', 'see', 'it', '.', 'now', 'onto', 'from', \"hell's\", 'appearance', ':', \"it's\", 'certainly', 'dark', 'and', 'bleak', 'enough', ',', 'and', \"it's\", 'surprising', 'to', 'see', 'how', 'much', 'more', 'it', 'looks', 'like', 'a', 'tim', 'burton', 'film', 'than', 'planet', 'of', 'the', 'apes', 'did', '(', 'at', 'times', ',', 'it', 'seems', 'like', 'sleepy', 'hollow', '2', ')', '.', 'the', 'print', 'i', 'saw', \"wasn't\", 'completely', 'finished', '(', 'both', 'color', 'and', 'music', 'had', 'not', 'been', 'finalized', ',', 'so', 'no', 'comments', 'about', 'marilyn', 'manson', ')', ',', 'but', 'cinematographer', 'peter', 'deming', '(', \"don't\", 'say', 'a', 'word', ')', 'ably', 'captures', 'the', 'dreariness', 'of', 'victorian-era', 'london', 'and', 'helped', 'make', 'the', 'flashy', 'killing', 'scenes', 'remind', 'me', 'of', 'the', 'crazy', 'flashbacks', 'in', 'twin', 'peaks', ',', 'even', 'though', 'the', 'violence', 'in', 'the', 'film', 'pales', 'in', 'comparison', 'to', 'that', 'in', 'the', 'black-and-white', 'comic', '.', 'oscar', 'winner', 'martin', \"childs'\", '(', 'shakespeare', 'in', 'love', ')', 'production', 'design', 'turns', 'the', 'original', 'prague', 'surroundings', 'into', 'one', 'creepy', 'place', '.', 'even', 'the', 'acting', 'in', 'from', 'hell', 'is', 'solid', ',', 'with', 'the', 'dreamy', 'depp', 'turning', 'in', 'a', 'typically', 'strong', 'performance', 'and', 'deftly', 'handling', 'a', 'british', 'accent', '.', 'ians', 'holm', '(', 'joe', \"gould's\", 'secret', ')', 'and', 'richardson', '(', '102', 'dalmatians', ')', 'log', 'in', 'great', 'supporting', 'roles', ',', 'but', 'the', 'big', 'surprise', 'here', 'is', 'graham', '.', 'i', 'cringed', 'the', 'first', 'time', 'she', 'opened', 'her', 'mouth', ',', 'imagining', 'her', 'attempt', 'at', 'an', 'irish', 'accent', ',', 'but', 'it', 'actually', \"wasn't\", 'half', 'bad', '.', 'the', 'film', ',', 'however', ',', 'is', 'all', 'good', '.', '2', ':', '00', '-', 'r', 'for', 'strong', 'violence/gore', ',', 'sexuality', ',', 'language', 'and', 'drug', 'content']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(movie_reviews[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['films', 'adapted', 'from', 'comic', 'books', 'have', 'had', 'plenty', 'of', 'success', 'whether', \"they're\", 'about', 'superheroes', 'batman', 'superman', 'spawn', 'or', 'geared', 'toward', 'kids', 'casper', 'or', 'the', 'arthouse', 'crowd', 'ghost', 'world', 'but', \"there's\", 'never', 'really', 'been', 'a', 'comic', 'book', 'like', 'from', 'hell', 'before', 'for', 'starters', 'it', 'was', 'created', 'by', 'alan', 'moore', 'and', 'eddie', 'campbell', 'who', 'brought', 'the', 'medium', 'to', 'a', 'whole', 'new', 'level', 'in', 'the', 'mid', \"'80s\", 'with', 'a', '12-part', 'series', 'called', 'the', 'watchmen', 'to', 'say', 'moore', 'and', 'campbell', 'thoroughly', 'researched', 'the', 'subject', 'of', 'jack', 'the', 'ripper', 'would', 'be', 'like', 'saying', 'michael', 'jackson', 'is', 'starting', 'to', 'look', 'a', 'little', 'odd', 'the', 'book', 'or', 'graphic', 'novel', 'if', 'you', 'will', 'is', 'over', '500', 'pages', 'long', 'and', 'includes', 'nearly', '30', 'more', 'that', 'consist', 'of', 'nothing', 'but', 'footnotes', 'in', 'other', 'words', \"don't\", 'dismiss', 'this', 'film', 'because', 'of', 'its', 'source', 'if', 'you', 'can', 'get', 'past', 'the', 'whole', 'comic', 'book', 'thing', 'you', 'might', 'find', 'another', 'stumbling', 'block', 'in', 'from', \"hell's\", 'directors', 'albert', 'and', 'allen', 'hughes', 'getting', 'the', 'hughes', 'brothers', 'to', 'direct', 'this', 'seems', 'almost', 'as', 'ludicrous', 'as', 'casting', 'carrot', 'top', 'in', 'well', 'anything', 'but', 'riddle', 'me', 'this', 'who', 'better', 'to', 'direct', 'a', 'film', \"that's\", 'set', 'in', 'the', 'ghetto', 'and', 'features', 'really', 'violent', 'street', 'crime', 'than', 'the', 'mad', 'geniuses', 'behind', 'menace', 'ii', 'society', 'the', 'ghetto', 'in', 'question', 'is', 'of', 'course', 'whitechapel', 'in', '1888', \"london's\", 'east', 'end', \"it's\", 'a', 'filthy', 'sooty', 'place', 'where', 'the', 'whores', 'called', 'unfortunates', 'are', 'starting', 'to', 'get', 'a', 'little', 'nervous', 'about', 'this', 'mysterious', 'psychopath', 'who', 'has', 'been', 'carving', 'through', 'their', 'profession', 'with', 'surgical', 'precision', 'when', 'the', 'first', 'stiff', 'turns', 'up', 'copper', 'peter', 'godley', 'robbie', 'coltrane', 'the', 'world', 'is', 'not', 'enough', 'calls', 'in', 'inspector', 'frederick', 'abberline', 'johnny', 'depp', 'blow', 'to', 'crack', 'the', 'case', 'abberline', 'a', 'widower', 'has', 'prophetic', 'dreams', 'he', 'unsuccessfully', 'tries', 'to', 'quell', 'with', 'copious', 'amounts', 'of', 'absinthe', 'and', 'opium', 'upon', 'arriving', 'in', 'whitechapel', 'he', 'befriends', 'an', 'unfortunate', 'named', 'mary', 'kelly', 'heather', 'graham', 'say', 'it', \"isn't\", 'so', 'and', 'proceeds', 'to', 'investigate', 'the', 'horribly', 'gruesome', 'crimes', 'that', 'even', 'the', 'police', 'surgeon', \"can't\", 'stomach', 'i', \"don't\", 'think', 'anyone', 'needs', 'to', 'be', 'briefed', 'on', 'jack', 'the', 'ripper', 'so', 'i', \"won't\", 'go', 'into', 'the', 'particulars', 'here', 'other', 'than', 'to', 'say', 'moore', 'and', 'campbell', 'have', 'a', 'unique', 'and', 'interesting', 'theory', 'about', 'both', 'the', 'identity', 'of', 'the', 'killer', 'and', 'the', 'reasons', 'he', 'chooses', 'to', 'slay', 'in', 'the', 'comic', 'they', \"don't\", 'bother', 'cloaking', 'the', 'identity', 'of', 'the', 'ripper', 'but', 'screenwriters', 'terry', 'hayes', 'vertical', 'limit', 'and', 'rafael', 'yglesias', 'les', 'mis', 'rables', 'do', 'a', 'good', 'job', 'of', 'keeping', 'him', 'hidden', 'from', 'viewers', 'until', 'the', 'very', 'end', \"it's\", 'funny', 'to', 'watch', 'the', 'locals', 'blindly', 'point', 'the', 'finger', 'of', 'blame', 'at', 'jews', 'and', 'indians', 'because', 'after', 'all', 'an', 'englishman', 'could', 'never', 'be', 'capable', 'of', 'committing', 'such', 'ghastly', 'acts', 'and', 'from', \"hell's\", 'ending', 'had', 'me', 'whistling', 'the', 'stonecutters', 'song', 'from', 'the', 'simpsons', 'for', 'days', 'who', 'holds', 'back', 'the', 'electric', 'car/who', 'made', 'steve', 'guttenberg', 'a', 'star', \"don't\", 'worry', \"it'll\", 'all', 'make', 'sense', 'when', 'you', 'see', 'it', 'now', 'onto', 'from', \"hell's\", 'appearance', \"it's\", 'certainly', 'dark', 'and', 'bleak', 'enough', 'and', \"it's\", 'surprising', 'to', 'see', 'how', 'much', 'more', 'it', 'looks', 'like', 'a', 'tim', 'burton', 'film', 'than', 'planet', 'of', 'the', 'apes', 'did', 'at', 'times', 'it', 'seems', 'like', 'sleepy', 'hollow', '2', 'the', 'print', 'i', 'saw', \"wasn't\", 'completely', 'finished', 'both', 'color', 'and', 'music', 'had', 'not', 'been', 'finalized', 'so', 'no', 'comments', 'about', 'marilyn', 'manson', 'but', 'cinematographer', 'peter', 'deming', \"don't\", 'say', 'a', 'word', 'ably', 'captures', 'the', 'dreariness', 'of', 'victorian-era', 'london', 'and', 'helped', 'make', 'the', 'flashy', 'killing', 'scenes', 'remind', 'me', 'of', 'the', 'crazy', 'flashbacks', 'in', 'twin', 'peaks', 'even', 'though', 'the', 'violence', 'in', 'the', 'film', 'pales', 'in', 'comparison', 'to', 'that', 'in', 'the', 'black-and-white', 'comic', 'oscar', 'winner', 'martin', \"childs'\", 'shakespeare', 'in', 'love', 'production', 'design', 'turns', 'the', 'original', 'prague', 'surroundings', 'into', 'one', 'creepy', 'place', 'even', 'the', 'acting', 'in', 'from', 'hell', 'is', 'solid', 'with', 'the', 'dreamy', 'depp', 'turning', 'in', 'a', 'typically', 'strong', 'performance', 'and', 'deftly', 'handling', 'a', 'british', 'accent', 'ians', 'holm', 'joe', \"gould's\", 'secret', 'and', 'richardson', '102', 'dalmatians', 'log', 'in', 'great', 'supporting', 'roles', 'but', 'the', 'big', 'surprise', 'here', 'is', 'graham', 'i', 'cringed', 'the', 'first', 'time', 'she', 'opened', 'her', 'mouth', 'imagining', 'her', 'attempt', 'at', 'an', 'irish', 'accent', 'but', 'it', 'actually', \"wasn't\", 'half', 'bad', 'the', 'film', 'however', 'is', 'all', 'good', '2', '00', 'r', 'for', 'strong', 'violence/gore', 'sexuality', 'language', 'and', 'drug', 'content']\n"
     ]
    }
   ],
   "source": [
    "print(preprocessed_movie_reviews[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fYf1LwMRlXd3"
   },
   "source": [
    "# 3. Topic Modelling using Gensim\n",
    "\n",
    "Gensim offers an implementation of Latent Dirichlet Allocation (LDA), the most popular topic modelling algorithm, which we discussed in the lecture. If you are working on Google Colab, it is normally already installed there. Otherwise, you can install it with `pip install --upgrade gensim` or if you are using Conda, `conda install -c conda-forge gensim`.\n",
    "\n",
    "Let's load it, and some other things we use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "w3KtsubolXd3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.3.1\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "import gensim.models as models\n",
    "import itertools\n",
    "from operator import itemgetter\n",
    "print(gensim.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pOrYQV90lXd6"
   },
   "source": [
    "## Constructing the bag-of-words model\n",
    "\n",
    "The `gensim.corpora.Dictionary()` class allows you to map words to numbers, which is what we need to make a bag-of-words model. In particular, the doc2bow() function converts a collection of words to a bag-of-words representation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "OXwSQdqRtUx6"
   },
   "outputs": [],
   "source": [
    "movie_dictionary = corpora.Dictionary(movie_reviews)\n",
    "movie_bow_corpus = [movie_dictionary.doc2bow(d) for d in movie_reviews]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what happened:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique tokens in the dataset: 50920\n",
      "\n",
      "The first 11 words in the bag-of-words model:\n",
      "{'\"': 0, \"'80s\": 1, '(': 2, ')': 3, ',': 4, '-': 5, '.': 6, '00': 7, '102': 8, '12-part': 9, '1888': 10, '2': 11}\n",
      "\n",
      "The start of the first review:\n",
      "['films', 'adapted', 'from', 'comic', 'books', 'have', 'had', 'plenty', 'of', 'success', ',', 'whether', \"they're\", 'about', 'superheroes', '(', 'batman', ',', 'superman', ',', 'spawn', ')', ',', 'or', 'geared', 'toward', 'kids', '(', 'casper', ')', 'or', 'the', 'arthouse', 'crowd', '(', 'ghost', 'world', ')', ',', 'but', \"there's\", 'never', 'really', 'been', 'a', 'comic', 'book', 'like', 'from', 'hell', 'before', '.', 'for', 'starters', ',', 'it', 'was', 'created', 'by', 'alan', 'moore', '(', 'and', 'eddie', 'campbell', ')', ',', 'who', 'brought', 'the', 'medium', 'to', 'a', 'whole', 'new', 'level', 'in', 'the', 'mid', \"'80s\", 'with', 'a', '12-part', 'series', 'called', 'the', 'watchmen', '.', 'to', 'say', 'moore', 'and', 'campbell', 'thoroughly', 'researched', 'the', 'subject', 'of', 'jack', 'the']\n",
      "\n",
      "The filename of the first review:\n",
      "cv000_29590.txt\n",
      "\n",
      "Most frequent words in the first review:\n",
      "the --> 46\n",
      ", --> 43\n",
      ". --> 23\n",
      "and --> 20\n",
      "( --> 18\n",
      ") --> 18\n",
      "in --> 18\n",
      "a --> 15\n",
      "to --> 15\n",
      "of --> 14\n",
      "from --> 8\n",
      "but --> 7\n",
      "is --> 7\n",
      "\" --> 6\n",
      "it --> 6\n",
      "comic --> 5\n",
      "don't --> 5\n",
      "film --> 5\n",
      "about --> 4\n",
      "i --> 4\n",
      "...\n"
     ]
    }
   ],
   "source": [
    "print('Number of unique tokens in the dataset:', len(movie_dictionary))\n",
    "\n",
    "#Checking the first 11 words in the bag-of-words model\n",
    "print('\\nThe first 11 words in the bag-of-words model:')\n",
    "print(dict(itertools.islice(movie_dictionary.token2id.items(), 12)))\n",
    "\n",
    "#Checking the first 100 words of the first review\n",
    "print('\\nThe start of the first review:')\n",
    "print(movie_reviews[0][:100])\n",
    "#And the filename of that review is...\n",
    "print('\\nThe filename of the first review:')\n",
    "print(movie_reviewnames[0])\n",
    "\n",
    "#Which words are used in that review?\n",
    "print('\\nMost frequent words in the first review:')\n",
    "for i, freq in sorted(movie_bow_corpus[0], key=itemgetter(1), reverse=True)[:20]:\n",
    "    print(movie_dictionary[i], \"-->\", freq)\n",
    "print(\"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The topic model\n",
    "\n",
    "Now, we can train our LDA model on this bag-of-words data by using `gensim.models.ldamodel.LDAModel()`.\n",
    "\n",
    "This model can take various parameters that specify what kind of model gets made. Some important ones:\n",
    "\n",
    "\n",
    "* num_topics: how many topics do we want? In what follows, we set the number of topics to 5, because we want to have a few topics that we can interpret, but the number of topics is data and application-dependent;\n",
    "* id2word: our bag-of-words dictionary needed to map ids to strings;\n",
    "* passes: how often we iterate over the entire corpus (default = 1). In general, the more passes, the higher the accuracy. This number is also called epochs in Artificial Intelligence and Machine Learning.\n",
    "\n",
    "Let's first make a model that finds 5 topics, and tries 25 times to improve its estimate. This code may take a while to run, as it is the process that creates the topic model. If it takes too long, you can reduce the number of passes, but the topics might be worse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "id": "UkT4re_Muowp"
   },
   "outputs": [],
   "source": [
    "reviews_ldamodel = models.ldamodel.LdaModel(movie_bow_corpus, num_topics=5, id2word = movie_dictionary, passes=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's have a look! An easy way to inspect the created topics is by using the `show_topics()` method, which prints the most representative word for each topic along with their probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.051*\",\" + 0.049*\"the\" + 0.045*\".\" + 0.028*\"a\" + 0.024*\"of\" + 0.023*\"and\" + 0.021*\"to\" + 0.017*\"is\"'),\n",
       " (1,\n",
       "  '0.000*\",\" + 0.000*\".\" + 0.000*\"to\" + 0.000*\"the\" + 0.000*\"of\" + 0.000*\"a\" + 0.000*\"and\" + 0.000*\"\"\"'),\n",
       " (2,\n",
       "  '0.001*\"caveman\\'s\" + 0.001*\"valentine_\" + 0.001*\"_the\" + 0.001*\"mouse\" + 0.001*\"romulus\" + 0.000*\"ghostface\" + 0.000*\"ghostface\\'s\" + 0.000*\"homeless\"'),\n",
       " (3,\n",
       "  '0.056*\",\" + 0.054*\"the\" + 0.040*\".\" + 0.026*\"and\" + 0.024*\"a\" + 0.023*\"of\" + 0.021*\"to\" + 0.018*\"is\"'),\n",
       " (4,\n",
       "  '0.048*\".\" + 0.045*\"the\" + 0.044*\",\" + 0.023*\"a\" + 0.021*\"to\" + 0.020*\"and\" + 0.019*\"of\" + 0.014*\"is\"')]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_ldamodel.show_topics(num_words=8) #Show the top 8 words for each topic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There we go, we have a topic model. However, you can probably see that it is far from perfect and some uninteresting 'words' appear there. Now, it is your turn to make it better!\n",
    "\n",
    "## Your model\n",
    "\n",
    "**Part 1: Preprocessing**\n",
    "\n",
    "Show the effect of your preprocessing by also making a topic model for your preprocessed_movie_reviews. First, you make a bag-of-words model and then the LdaModel, as above. Feel free to go back to your preprocessing code above and update it based on what you saw from the show_topics function applied to the initial model.\n",
    "\n",
    "Try to make a model with 8 topics, and show the top 8 words for each topic. **Assign the model to a new variable with a sensible name** (avoid overwriting the previous models).\n",
    "\n",
    "Also for the dictionary and corpus, **give the variables different and expressive names to avoid overwriting the other ones**. Otherwise, you will get confused between your different topic models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique tokens in the dataset: 50893\n",
      "\n",
      "The first 11 words in the bag-of-words model:\n",
      "{\"'80s\": 0, '00': 1, '102': 2, '12-part': 3, '1888': 4, '2': 5, '30': 6, '500': 7, 'a': 8, 'abberline': 9, 'ably': 10, 'about': 11}\n",
      "\n",
      "The start of the first review:\n",
      "['films', 'adapted', 'from', 'comic', 'books', 'have', 'had', 'plenty', 'of', 'success', 'whether', \"they're\", 'about', 'superheroes', 'batman', 'superman', 'spawn', 'or', 'geared', 'toward', 'kids', 'casper', 'or', 'the', 'arthouse', 'crowd', 'ghost', 'world', 'but', \"there's\", 'never', 'really', 'been', 'a', 'comic', 'book', 'like', 'from', 'hell', 'before', 'for', 'starters', 'it', 'was', 'created', 'by', 'alan', 'moore', 'and', 'eddie', 'campbell', 'who', 'brought', 'the', 'medium', 'to', 'a', 'whole', 'new', 'level', 'in', 'the', 'mid', \"'80s\", 'with', 'a', '12-part', 'series', 'called', 'the', 'watchmen', 'to', 'say', 'moore', 'and', 'campbell', 'thoroughly', 'researched', 'the', 'subject', 'of', 'jack', 'the', 'ripper', 'would', 'be', 'like', 'saying', 'michael', 'jackson', 'is', 'starting', 'to', 'look', 'a', 'little', 'odd', 'the', 'book', 'or']\n",
      "\n",
      "The filename of the first review:\n",
      "cv000_29590.txt\n",
      "\n",
      "Most frequent words in the first review:\n",
      "the --> 46\n",
      "and --> 20\n",
      "in --> 18\n",
      "a --> 15\n",
      "to --> 15\n",
      "of --> 14\n",
      "from --> 8\n",
      "but --> 7\n",
      "is --> 7\n",
      "it --> 6\n",
      "comic --> 5\n",
      "don't --> 5\n",
      "film --> 5\n",
      "about --> 4\n",
      "i --> 4\n",
      "it's --> 4\n",
      "like --> 4\n",
      "say --> 4\n",
      "this --> 4\n",
      "who --> 4\n",
      "...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.015*\"the\" + 0.009*\"and\" + 0.008*\"a\" + 0.006*\"to\" + 0.006*\"of\" + 0.004*\"is\" + 0.004*\"wars\" + 0.004*\"her\"'),\n",
       " (1,\n",
       "  '0.027*\"the\" + 0.022*\"and\" + 0.017*\"a\" + 0.012*\"of\" + 0.011*\"to\" + 0.010*\"i\" + 0.009*\"is\" + 0.008*\"that\"'),\n",
       " (2,\n",
       "  '0.053*\"the\" + 0.030*\"and\" + 0.028*\"a\" + 0.026*\"of\" + 0.023*\"to\" + 0.022*\"is\" + 0.017*\"in\" + 0.014*\"his\"'),\n",
       " (3,\n",
       "  '0.060*\"the\" + 0.031*\"of\" + 0.027*\"a\" + 0.026*\"and\" + 0.021*\"to\" + 0.015*\"is\" + 0.014*\"in\" + 0.009*\"as\"'),\n",
       " (4,\n",
       "  '0.060*\"the\" + 0.030*\"a\" + 0.026*\"and\" + 0.025*\"of\" + 0.025*\"to\" + 0.019*\"is\" + 0.017*\"in\" + 0.013*\"that\"')]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed_movie_dictionary = corpora.Dictionary(preprocessed_movie_reviews)\n",
    "preprocessed_movie_bow_corpus = [preprocessed_movie_dictionary.doc2bow(d) for d in preprocessed_movie_reviews]\n",
    "\n",
    "print('Number of unique tokens in the dataset:', len(preprocessed_movie_dictionary))\n",
    "\n",
    "#Checking the first 11 words in the bag-of-words model\n",
    "print('\\nThe first 11 words in the bag-of-words model:')\n",
    "print(dict(itertools.islice(preprocessed_movie_dictionary.token2id.items(), 12)))\n",
    "\n",
    "#Checking the first 100 words of the first review\n",
    "print('\\nThe start of the first review:')\n",
    "print(preprocessed_movie_reviews[0][:100])\n",
    "#And the filename of that review is...\n",
    "print('\\nThe filename of the first review:')\n",
    "print(movie_reviewnames[0])\n",
    "\n",
    "#Which words are used in that review?\n",
    "print('\\nMost frequent words in the first review:')\n",
    "for i, freq in sorted(preprocessed_movie_bow_corpus[0], key=itemgetter(1), reverse=True)[:20]:\n",
    "    print(preprocessed_movie_dictionary[i], \"-->\", freq)\n",
    "print(\"...\")\n",
    "\n",
    "preprocessed_reviews_ldamodel = models.ldamodel.LdaModel(preprocessed_movie_bow_corpus, num_topics=5, id2word = preprocessed_movie_dictionary, passes=25)\n",
    "\n",
    "preprocessed_reviews_ldamodel.show_topics(num_words=8) #Show the top 8 words for each topic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part 2: Stopwords**\n",
    "\n",
    "The topics you saw so far are probably mostly made up of stopwords such as \"the\". As discussed in the lecture, our results will probably be more interesting if we get rid of them.\n",
    "\n",
    "We have included 3 generic lists of stopwords: the default list of the tool Mallet, a shorter frequent word list used in search applications (Snowball stemmer), and the top 10,000 words based on Google n-grams (in frequency order, select as many lines as you want). Gensim and NLTK also have stopword lists.\n",
    "\n",
    "Make a function that accepts the path to a stopwords file (e.g. `stopwords/standard-mallet-en.txt`), and returns a list of stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def load_stopwords(filename): \n",
    "    with open(filename, 'r') as file:\n",
    "        contents = file.read()\n",
    "        \n",
    "        stopword_list = contents.split()\n",
    "    \n",
    "    return stopword_list\n",
    "\n",
    "stopword_list = load_stopwords(\"stopwords/standard-mallet-en.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, make a function that takes a stopword list and a list of reviews (e.g. `preprocessed_movie_reviews`). The function should remove all stopwords from all the reviews, returning a list of the reviews without stopwords. This code may be a bit slow if you have many stopwords, since there is a lot of data to process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_stopwords(stopword_list, movie_reviews):\n",
    "\n",
    "    filtered_movie_reviews = []\n",
    "    for review in movie_reviews:\n",
    "        normalized = []\n",
    "        for token in review:\n",
    "            if token in stopword_list:\n",
    "                continue\n",
    "            else:\n",
    "                normalized.append(token)\n",
    "        filtered_movie_reviews.append(normalized)\n",
    "        \n",
    "    return filtered_movie_reviews\n",
    "\n",
    "filtered_movie_reviews = filter_stopwords(stopword_list, preprocessed_movie_reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, let's make another topic model with this filtered data! Again, you make a bag-of-words model and then the LdaModel, as above.\n",
    "\n",
    "Try to make a model with 8 topics, and show the top 8 words for each topic. Assign the model to a new variable with a sensible name (avoid overwriting the previous models)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique tokens in the dataset: 50393\n",
      "\n",
      "The first 11 words in the bag-of-words model:\n",
      "{\"'80s\": 0, '00': 1, '102': 2, '12-part': 3, '1888': 4, '2': 5, '30': 6, '500': 7, 'abberline': 8, 'ably': 9, 'absinthe': 10, 'accent': 11}\n",
      "\n",
      "The start of the first review:\n",
      "['films', 'adapted', 'comic', 'books', 'plenty', 'success', \"they're\", 'superheroes', 'batman', 'superman', 'spawn', 'geared', 'kids', 'casper', 'arthouse', 'crowd', 'ghost', 'world', \"there's\", 'comic', 'book', 'hell', 'starters', 'created', 'alan', 'moore', 'eddie', 'campbell', 'brought', 'medium', 'level', 'mid', \"'80s\", '12-part', 'series', 'called', 'watchmen', 'moore', 'campbell', 'researched', 'subject', 'jack', 'ripper', 'michael', 'jackson', 'starting', 'odd', 'book', 'graphic', '500', 'pages', 'long', 'includes', '30', 'consist', 'footnotes', 'words', \"don't\", 'dismiss', 'film', 'source', 'past', 'comic', 'book', 'thing', 'find', 'stumbling', 'block', \"hell's\", 'directors', 'albert', 'allen', 'hughes', 'hughes', 'brothers', 'direct', 'ludicrous', 'casting', 'carrot', 'top', 'riddle', 'direct', 'film', \"that's\", 'set', 'ghetto', 'features', 'violent', 'street', 'crime', 'mad', 'geniuses', 'menace', 'ii', 'society', 'ghetto', 'question', 'whitechapel', '1888', \"london's\"]\n",
      "\n",
      "The filename of the first review:\n",
      "cv000_29590.txt\n",
      "\n",
      "Most frequent words in the first review:\n",
      "comic --> 5\n",
      "don't --> 5\n",
      "film --> 5\n",
      "it's --> 4\n",
      "book --> 3\n",
      "campbell --> 3\n",
      "hell's --> 3\n",
      "moore --> 3\n",
      "ripper --> 3\n",
      "2 --> 2\n",
      "abberline --> 2\n",
      "accent --> 2\n",
      "called --> 2\n",
      "depp --> 2\n",
      "direct --> 2\n",
      "end --> 2\n",
      "ghetto --> 2\n",
      "good --> 2\n",
      "graham --> 2\n",
      "hell --> 2\n",
      "...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.014*\"film\" + 0.011*\"movie\" + 0.007*\"it\\'s\" + 0.004*\"time\" + 0.004*\"good\" + 0.004*\"story\" + 0.003*\"character\" + 0.003*\"--\"'),\n",
       " (1,\n",
       "  '0.014*\"film\" + 0.012*\"movie\" + 0.006*\"it\\'s\" + 0.004*\"good\" + 0.004*\"bad\" + 0.004*\"time\" + 0.003*\"plot\" + 0.003*\"character\"'),\n",
       " (2,\n",
       "  '0.014*\"film\" + 0.004*\"movie\" + 0.003*\"it\\'s\" + 0.003*\"star\" + 0.003*\"time\" + 0.003*\"characters\" + 0.003*\"story\" + 0.003*\"good\"'),\n",
       " (3,\n",
       "  '0.013*\"film\" + 0.006*\"movie\" + 0.006*\"it\\'s\" + 0.004*\"story\" + 0.004*\"good\" + 0.004*\"time\" + 0.003*\"character\" + 0.003*\"life\"'),\n",
       " (4,\n",
       "  '0.015*\"film\" + 0.007*\"movie\" + 0.005*\"it\\'s\" + 0.003*\"characters\" + 0.003*\"time\" + 0.003*\"good\" + 0.003*\"character\" + 0.003*\"story\"')]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_movie_dictionary = corpora.Dictionary(filtered_movie_reviews)\n",
    "filtered_movie_bow_corpus = [filtered_movie_dictionary.doc2bow(d) for d in filtered_movie_reviews]\n",
    "\n",
    "print('Number of unique tokens in the dataset:', len(filtered_movie_dictionary))\n",
    "\n",
    "#Checking the first 11 words in the bag-of-words model\n",
    "print('\\nThe first 11 words in the bag-of-words model:')\n",
    "print(dict(itertools.islice(filtered_movie_dictionary.token2id.items(), 12)))\n",
    "\n",
    "#Checking the first 100 words of the first review\n",
    "print('\\nThe start of the first review:')\n",
    "print(filtered_movie_reviews[0][:100])\n",
    "#And the filename of that review is...\n",
    "print('\\nThe filename of the first review:')\n",
    "print(movie_reviewnames[0])\n",
    "\n",
    "#Which words are used in that review?\n",
    "print('\\nMost frequent words in the first review:')\n",
    "for i, freq in sorted(filtered_movie_bow_corpus[0], key=itemgetter(1), reverse=True)[:20]:\n",
    "    print(filtered_movie_dictionary[i], \"-->\", freq)\n",
    "print(\"...\")\n",
    "\n",
    "filtered_reviews_ldamodel = models.ldamodel.LdaModel(filtered_movie_bow_corpus, num_topics=5, id2word = filtered_movie_dictionary, passes=25)\n",
    "\n",
    "filtered_reviews_ldamodel.show_topics(num_words=8) #Show the top 8 words for each topic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part 3: Experimentation**\n",
    "\n",
    "Are these general stopword lists sufficient? We are working in the movie review domain, meaning that we may have other uninformative stopwords than in the general domain, such as the word 'movie'. Some key experimentation is to add specific stopwords for the movie review domain, which would occur frequently in all (or most) of the clusters. Note that removing words will not just hide these words, but lead to (even very) different topics and different top ranked reviews.\n",
    "\n",
    "**Make your own domain-specific stopwords file** by taking one of the existing ones and adding your own stopwords (make sure that the stopword file is saved as a plain text file). Think about what stopwords are in this domain (e.g., the word film is not a stopword in general, but it will occur in essentially every film review).\n",
    "\n",
    "Re-use the functions you previously made to load your own stopwords file and filter the movie reviews. Then, make another topic model with your new filtering and show the top 8 words for each topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_stopword_list = load_stopwords(\"stopwords/my_movie_stopwords.txt\")\n",
    "domainfiltered_movie_reviews = filter_stopwords(my_stopword_list, preprocessed_movie_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique tokens in the dataset: 50369\n",
      "\n",
      "The first 11 words in the bag-of-words model:\n",
      "{\"'80s\": 0, '00': 1, '102': 2, '12-part': 3, '1888': 4, '2': 5, '30': 6, '500': 7, 'abberline': 8, 'ably': 9, 'absinthe': 10, 'accent': 11}\n",
      "\n",
      "The start of the first review:\n",
      "['adapted', 'comic', 'books', 'plenty', 'success', \"they're\", 'superheroes', 'batman', 'superman', 'spawn', 'geared', 'kids', 'casper', 'arthouse', 'crowd', 'ghost', 'world', 'comic', 'book', 'hell', 'starters', 'created', 'alan', 'moore', 'eddie', 'campbell', 'brought', 'medium', 'level', 'mid', \"'80s\", '12-part', 'series', 'called', 'watchmen', 'moore', 'campbell', 'researched', 'subject', 'jack', 'ripper', 'michael', 'jackson', 'starting', 'odd', 'book', 'graphic', '500', 'pages', 'long', 'includes', '30', 'consist', 'footnotes', 'words', 'dismiss', 'source', 'past', 'comic', 'book', 'thing', 'find', 'stumbling', 'block', \"hell's\", 'directors', 'albert', 'allen', 'hughes', 'hughes', 'brothers', 'direct', 'ludicrous', 'casting', 'carrot', 'top', 'riddle', 'direct', \"that's\", 'set', 'ghetto', 'features', 'violent', 'street', 'crime', 'mad', 'geniuses', 'menace', 'ii', 'society', 'ghetto', 'question', 'whitechapel', '1888', \"london's\", 'east', 'end', 'filthy', 'sooty', 'place']\n",
      "\n",
      "The filename of the first review:\n",
      "cv000_29590.txt\n",
      "\n",
      "Most frequent words in the first review:\n",
      "comic --> 5\n",
      "book --> 3\n",
      "campbell --> 3\n",
      "hell's --> 3\n",
      "moore --> 3\n",
      "ripper --> 3\n",
      "2 --> 2\n",
      "abberline --> 2\n",
      "accent --> 2\n",
      "called --> 2\n",
      "depp --> 2\n",
      "direct --> 2\n",
      "end --> 2\n",
      "ghetto --> 2\n",
      "graham --> 2\n",
      "hell --> 2\n",
      "hughes --> 2\n",
      "identity --> 2\n",
      "jack --> 2\n",
      "peter --> 2\n",
      "...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.003*\"life\" + 0.002*\"man\" + 0.002*\"people\" + 0.002*\"love\" + 0.002*\"work\" + 0.002*\"real\" + 0.002*\"comedy\" + 0.002*\"scream\"'),\n",
       " (1,\n",
       "  '0.003*\"people\" + 0.002*\"end\" + 0.002*\"man\" + 0.002*\"love\" + 0.002*\"audience\" + 0.002*\"life\" + 0.002*\"fact\" + 0.002*\"performance\"'),\n",
       " (2,\n",
       "  '0.003*\"life\" + 0.002*\"big\" + 0.002*\"man\" + 0.002*\"people\" + 0.002*\"love\" + 0.002*\"made\" + 0.002*\"back\" + 0.002*\"funny\"'),\n",
       " (3,\n",
       "  '0.003*\"action\" + 0.003*\"people\" + 0.002*\"life\" + 0.002*\"man\" + 0.002*\"end\" + 0.002*\"role\" + 0.002*\"alien\" + 0.002*\"back\"'),\n",
       " (4,\n",
       "  '0.002*\"world\" + 0.002*\"people\" + 0.002*\"life\" + 0.002*\"makes\" + 0.002*\"man\" + 0.002*\"made\" + 0.002*\"work\" + 0.002*\"action\"')]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "domainfiltered_movie_dictionary = corpora.Dictionary(domainfiltered_movie_reviews)\n",
    "domainfiltered_movie_bow_corpus = [domainfiltered_movie_dictionary.doc2bow(d) for d in domainfiltered_movie_reviews]\n",
    "\n",
    "print('Number of unique tokens in the dataset:', len(domainfiltered_movie_dictionary))\n",
    "\n",
    "#Checking the first 11 words in the bag-of-words model\n",
    "print('\\nThe first 11 words in the bag-of-words model:')\n",
    "print(dict(itertools.islice(domainfiltered_movie_dictionary.token2id.items(), 12)))\n",
    "\n",
    "#Checking the first 100 words of the first review\n",
    "print('\\nThe start of the first review:')\n",
    "print(domainfiltered_movie_reviews[0][:100])\n",
    "#And the filename of that review is...\n",
    "print('\\nThe filename of the first review:')\n",
    "print(movie_reviewnames[0])\n",
    "\n",
    "#Which words are used in that review?\n",
    "print('\\nMost frequent words in the first review:')\n",
    "for i, freq in sorted(domainfiltered_movie_bow_corpus[0], key=itemgetter(1), reverse=True)[:20]:\n",
    "    print(domainfiltered_movie_dictionary[i], \"-->\", freq)\n",
    "print(\"...\")\n",
    "\n",
    "domainfiltered_reviews_ldamodel = models.ldamodel.LdaModel(domainfiltered_movie_bow_corpus, num_topics=5, id2word = domainfiltered_movie_dictionary, passes=25)\n",
    "\n",
    "domainfiltered_reviews_ldamodel.show_topics(num_words=8) #Show the top 8 words for each topic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you should have 3 models (or more): one without any stopword filtering, one with the standard stopword filtering and one with the domain-filtered stopwords using the list you modified yourself. Compare the topics found by the three models (just looking at them is fine, no need to code a comparison).\n",
    "\n",
    "Do the topics look better with stopword filtering and with domain-specific stopword filtering? At this point, do the resulting topics correspond to particular film genres you have expected?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course the results look better with stopwords filtering. Without filtering there is puncuation, articles, and repetitive words from the sentences which do not add any value to the topic model. Removing more generic words leads to a more precise topic genre."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Increase the number of topics. What happens with the topics if you model very few or very many topics? (answer in a text box). Assign the model(s) to a new variable with a sensible name (avoid overwriting the previous models)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2,\n",
       "  '0.003*\"people\" + 0.002*\"funny\" + 0.002*\"life\" + 0.002*\"man\" + 0.002*\"real\" + 0.002*\"comedy\" + 0.002*\"big\" + 0.002*\"love\"'),\n",
       " (0,\n",
       "  '0.004*\"life\" + 0.004*\"scream\" + 0.003*\"2\" + 0.003*\"mulan\" + 0.003*\"toy\" + 0.003*\"disney\" + 0.003*\"spice\" + 0.002*\"horror\"'),\n",
       " (6,\n",
       "  '0.003*\"man\" + 0.002*\"big\" + 0.002*\"john\" + 0.002*\"world\" + 0.002*\"action\" + 0.002*\"murphy\" + 0.002*\"role\" + 0.002*\"makes\"'),\n",
       " (8,\n",
       "  '0.003*\"man\" + 0.002*\"apes\" + 0.002*\"people\" + 0.002*\"witch\" + 0.002*\"love\" + 0.002*\"action\" + 0.002*\"blair\" + 0.002*\"funny\"'),\n",
       " (14,\n",
       "  '0.004*\"ryan\" + 0.003*\"life\" + 0.003*\"tarzan\" + 0.003*\"big\" + 0.003*\"war\" + 0.002*\"man\" + 0.002*\"city\" + 0.002*\"back\"'),\n",
       " (1,\n",
       "  '0.002*\"man\" + 0.002*\"action\" + 0.002*\"made\" + 0.002*\"love\" + 0.002*\"real\" + 0.002*\"original\" + 0.002*\"people\" + 0.002*\"thing\"'),\n",
       " (12,\n",
       "  '0.004*\"star\" + 0.003*\"trek\" + 0.003*\"godzilla\" + 0.003*\"people\" + 0.002*\"effects\" + 0.002*\"special\" + 0.002*\"actors\" + 0.002*\"work\"'),\n",
       " (9,\n",
       "  '0.003*\"truman\" + 0.003*\"action\" + 0.003*\"carrey\" + 0.002*\"men\" + 0.002*\"man\" + 0.002*\"end\" + 0.002*\"life\" + 0.002*\"performance\"'),\n",
       " (3,\n",
       "  '0.004*\"life\" + 0.003*\"people\" + 0.002*\"ship\" + 0.002*\"end\" + 0.002*\"real\" + 0.002*\"back\" + 0.002*\"audience\" + 0.002*\"made\"'),\n",
       " (10,\n",
       "  '0.004*\"action\" + 0.003*\"star\" + 0.003*\"people\" + 0.002*\"effects\" + 0.002*\"end\" + 0.002*\"role\" + 0.002*\"that\\'s\" + 0.002*\"made\"')]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "domainfiltered_reviews_ldamodel_extended = models.ldamodel.LdaModel(domainfiltered_movie_bow_corpus, num_topics=15, id2word = domainfiltered_movie_dictionary, passes=25)\n",
    "\n",
    "domainfiltered_reviews_ldamodel_extended.show_topics(num_words=8) #Show the top 8 words for each topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.003*\"life\" + 0.002*\"man\" + 0.002*\"love\" + 0.002*\"people\" + 0.002*\"end\" + 0.002*\"made\" + 0.002*\"performance\" + 0.002*\"work\"'),\n",
       " (1,\n",
       "  '0.003*\"people\" + 0.002*\"life\" + 0.002*\"action\" + 0.002*\"big\" + 0.002*\"man\" + 0.002*\"back\" + 0.002*\"world\" + 0.002*\"effects\"')]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "domainfiltered_reviews_ldamodel_reduced = models.ldamodel.LdaModel(domainfiltered_movie_bow_corpus, num_topics=2, id2word = domainfiltered_movie_dictionary, passes=25)\n",
    "\n",
    "domainfiltered_reviews_ldamodel_reduced.show_topics(num_words=8) #Show the top 8 words for each topic"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modelling less topics shows topics for less movie reviews which means that less information is presented. No difference in the speed though."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Increase the number of topic words printed to get more information per topic.  Is it easier to make sense of a topic if you look further down the list, or are the initial words more clear?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2,\n",
       "  '0.003*\"funny\" + 0.002*\"life\" + 0.002*\"action\" + 0.002*\"comedy\" + 0.002*\"makes\" + 0.002*\"bob\" + 0.002*\"smith\" + 0.002*\"man\" + 0.002*\"family\" + 0.002*\"long\" + 0.002*\"jay\" + 0.002*\"run\" + 0.002*\"people\" + 0.002*\"show\"'),\n",
       " (3,\n",
       "  '0.004*\"action\" + 0.003*\"star\" + 0.003*\"life\" + 0.002*\"jackie\" + 0.002*\"people\" + 0.002*\"wars\" + 0.002*\"that\\'s\" + 0.002*\"back\" + 0.002*\"guy\" + 0.002*\"effects\" + 0.002*\"end\" + 0.002*\"script\" + 0.002*\"series\" + 0.002*\"special\"'),\n",
       " (6,\n",
       "  '0.003*\"end\" + 0.003*\"back\" + 0.003*\"funny\" + 0.002*\"people\" + 0.002*\"action\" + 0.002*\"evil\" + 0.002*\"script\" + 0.002*\"wild\" + 0.002*\"work\" + 0.002*\"thing\" + 0.002*\"life\" + 0.002*\"things\" + 0.002*\"audience\" + 0.002*\"man\"'),\n",
       " (13,\n",
       "  '0.004*\"love\" + 0.003*\"people\" + 0.003*\"man\" + 0.002*\"life\" + 0.002*\"things\" + 0.002*\"world\" + 0.002*\"big\" + 0.002*\"made\" + 0.002*\"makes\" + 0.002*\"comedy\" + 0.002*\"young\" + 0.002*\"men\" + 0.002*\"performance\" + 0.002*\"end\"'),\n",
       " (8,\n",
       "  '0.003*\"action\" + 0.003*\"people\" + 0.003*\"alien\" + 0.002*\"big\" + 0.002*\"effects\" + 0.002*\"john\" + 0.002*\"war\" + 0.002*\"made\" + 0.002*\"aliens\" + 0.002*\"life\" + 0.002*\"world\" + 0.002*\"men\" + 0.002*\"man\" + 0.002*\"special\"'),\n",
       " (5,\n",
       "  '0.003*\"man\" + 0.002*\"people\" + 0.002*\"love\" + 0.002*\"life\" + 0.002*\"end\" + 0.002*\"makes\" + 0.002*\"disney\" + 0.002*\"work\" + 0.002*\"performance\" + 0.002*\"made\" + 0.002*\"audience\" + 0.002*\"family\" + 0.002*\"things\" + 0.002*\"sense\"'),\n",
       " (9,\n",
       "  '0.003*\"life\" + 0.003*\"star\" + 0.003*\"man\" + 0.002*\"end\" + 0.002*\"young\" + 0.002*\"trek\" + 0.002*\"people\" + 0.002*\"makes\" + 0.002*\"made\" + 0.002*\"back\" + 0.002*\"world\" + 0.002*\"role\" + 0.002*\"things\" + 0.002*\"work\"'),\n",
       " (4,\n",
       "  '0.003*\"life\" + 0.002*\"original\" + 0.002*\"made\" + 0.002*\"people\" + 0.002*\"back\" + 0.002*\"man\" + 0.002*\"godzilla\" + 0.002*\"work\" + 0.002*\"played\" + 0.002*\"years\" + 0.002*\"end\" + 0.002*\"thing\" + 0.002*\"audience\" + 0.002*\"love\"'),\n",
       " (10,\n",
       "  '0.003*\"people\" + 0.002*\"big\" + 0.002*\"action\" + 0.002*\"back\" + 0.002*\"end\" + 0.002*\"played\" + 0.002*\"work\" + 0.002*\"world\" + 0.002*\"real\" + 0.002*\"years\" + 0.002*\"life\" + 0.002*\"joe\" + 0.002*\"fact\" + 0.002*\"makes\"'),\n",
       " (14,\n",
       "  '0.004*\"life\" + 0.003*\"titanic\" + 0.003*\"ship\" + 0.002*\"love\" + 0.002*\"shrek\" + 0.002*\"funny\" + 0.002*\"made\" + 0.002*\"deep\" + 0.002*\"makes\" + 0.002*\"work\" + 0.002*\"people\" + 0.001*\"comedy\" + 0.001*\"cameron\" + 0.001*\"case\"')]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "domainfiltered_reviews_ldamodel_extended2 = models.ldamodel.LdaModel(domainfiltered_movie_bow_corpus, num_topics=15, id2word = domainfiltered_movie_dictionary, passes=25)\n",
    "\n",
    "domainfiltered_reviews_ldamodel_extended2.show_topics(num_words=14) #Show the top 8 words for each topic"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Up to a ceratin point it is easier to understand what kind of movie it is by looking at more topic words in the list. Yet looking at 100s of words would proabably not be better than maybe the 20 top words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are interested, you can also experiment with the difference between positive and negative reviews."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tnyB5hCglXeB",
    "tags": []
   },
   "source": [
    "### Part 4: Evaluation\n",
    "\n",
    "There are a few numbers we can compute that indicate the quality of a topic model, such as [perplexity and coherence](https://github.com/ccs-amsterdam/r-course-material/blob/master/tutorials/R_text_LDA_perplexity.md). For perplexity, a lower number means a better model, and for coherence, a higher number is better. Try computing these scores for your models, and see which is the best one according to the numbers\n",
    "\n",
    "In a real project, you should compute these numbers over a separate part of the dataset (the test set) for a proper evaluation, but for simplicity and because we have not talked about this in the lecture we will skip that here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import CoherenceModel\n",
    "\n",
    "def coherence_model_function(reviews_ldamodel, movie_bow_corpus, modelpoint, dictionarypoint ,textspoint):\n",
    "\n",
    "    #Compute perplexity for the basic model on the bag-of-words representation of the reviews:\n",
    "    print(f\"For {reviews_ldamodel}\")\n",
    "    \n",
    "    print('Perplexity: ', reviews_ldamodel.log_perplexity(movie_bow_corpus))  \n",
    "    #coherence_model_lda = {}\n",
    "    # Compute coherence score on the same:\n",
    "    coherence_model_lda = CoherenceModel(modelpoint, texts=textspoint, dictionary=dictionarypoint, coherence='c_v')\n",
    "    coherence_lda = coherence_model_lda.get_coherence()\n",
    "    print('Coherence score: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For LdaModel<num_terms=50920, num_topics=5, decay=0.5, chunksize=2000>\n",
      "Perplexity:  -7.012861000324164\n",
      "Coherence score:  0.24250900769765563\n"
     ]
    }
   ],
   "source": [
    "coherence_model_function(reviews_ldamodel, movie_bow_corpus, reviews_ldamodel, movie_dictionary ,movie_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For LdaModel<num_terms=50893, num_topics=5, decay=0.5, chunksize=2000>\n",
      "Perplexity:  -7.390783729632215\n",
      "Coherence score:  0.23951106201973132\n",
      "For LdaModel<num_terms=50393, num_topics=5, decay=0.5, chunksize=2000>\n",
      "Perplexity:  -9.20436684280963\n",
      "Coherence score:  0.27373985326920336\n"
     ]
    }
   ],
   "source": [
    "coherence_model_function(preprocessed_reviews_ldamodel, preprocessed_movie_bow_corpus, preprocessed_reviews_ldamodel, preprocessed_movie_dictionary ,preprocessed_movie_reviews)\n",
    "\n",
    "coherence_model_function(filtered_reviews_ldamodel, filtered_movie_bow_corpus, filtered_reviews_ldamodel, filtered_movie_dictionary ,filtered_movie_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For LdaModel<num_terms=50369, num_topics=5, decay=0.5, chunksize=2000>\n",
      "Perplexity:  -9.447892057157762\n",
      "Coherence score:  0.24760062733127483\n"
     ]
    }
   ],
   "source": [
    "coherence_model_function(domainfiltered_reviews_ldamodel, domainfiltered_movie_bow_corpus, domainfiltered_reviews_ldamodel, domainfiltered_movie_dictionary ,domainfiltered_movie_reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, just comparing numbers is not very interpretable. We will choose our topic model with the highest coherence score and validate the evaluation.\n",
    "\n",
    "Using the top 20 topic words for each topic in the model with the highest coherence score, pick at least 5 topic numbers and determine what film genres (in an informal sense) they represent, i.e. think of a meaningful label for the topic. Write down the topic number and your topic label. Is it easy to guess what the topic represents? For how many topics are you fairly confident, for how many do you have to make a guess, and for how many do you have no real clue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.002*\"scream\" + 0.002*\"love\" + 0.002*\"people\" + 0.002*\"action\" + 0.002*\"man\" + 0.002*\"makes\" + 0.002*\"life\" + 0.002*\"made\" + 0.002*\"real\" + 0.002*\"funny\" + 0.001*\"acting\" + 0.001*\"big\" + 0.001*\"end\" + 0.001*\"audience\" + 0.001*\"high\" + 0.001*\"show\" + 0.001*\"young\" + 0.001*\"back\" + 0.001*\"horror\" + 0.001*\"original\"'),\n",
       " (1,\n",
       "  '0.004*\"life\" + 0.002*\"star\" + 0.002*\"truman\" + 0.002*\"wars\" + 0.002*\"show\" + 0.002*\"people\" + 0.002*\"back\" + 0.002*\"action\" + 0.002*\"alien\" + 0.001*\"effects\" + 0.001*\"world\" + 0.001*\"end\" + 0.001*\"years\" + 0.001*\"young\" + 0.001*\"find\" + 0.001*\"lucas\" + 0.001*\"long\" + 0.001*\"ship\" + 0.001*\"work\" + 0.001*\"man\"'),\n",
       " (2,\n",
       "  '0.003*\"life\" + 0.002*\"people\" + 0.002*\"world\" + 0.002*\"love\" + 0.002*\"back\" + 0.002*\"years\" + 0.002*\"audience\" + 0.002*\"man\" + 0.002*\"work\" + 0.002*\"big\" + 0.002*\"star\" + 0.002*\"made\" + 0.002*\"things\" + 0.002*\"end\" + 0.002*\"john\" + 0.002*\"action\" + 0.001*\"effects\" + 0.001*\"makes\" + 0.001*\"young\" + 0.001*\"find\"'),\n",
       " (3,\n",
       "  '0.003*\"people\" + 0.003*\"man\" + 0.003*\"life\" + 0.002*\"end\" + 0.002*\"love\" + 0.002*\"action\" + 0.002*\"made\" + 0.002*\"back\" + 0.002*\"makes\" + 0.002*\"performance\" + 0.002*\"funny\" + 0.002*\"played\" + 0.002*\"work\" + 0.002*\"big\" + 0.002*\"real\" + 0.002*\"things\" + 0.002*\"plays\" + 0.002*\"role\" + 0.002*\"thing\" + 0.002*\"guy\"'),\n",
       " (4,\n",
       "  '0.002*\"man\" + 0.002*\"people\" + 0.002*\"work\" + 0.002*\"life\" + 0.002*\"john\" + 0.002*\"performance\" + 0.002*\"big\" + 0.002*\"made\" + 0.002*\"makes\" + 0.002*\"role\" + 0.002*\"back\" + 0.002*\"action\" + 0.001*\"end\" + 0.001*\"world\" + 0.001*\"cast\" + 0.001*\"city\" + 0.001*\"comedy\" + 0.001*\"long\" + 0.001*\"years\" + 0.001*\"actors\"')]"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "domainfiltered_reviews_ldamodel_highscore = models.ldamodel.LdaModel(domainfiltered_movie_bow_corpus, num_topics=5, id2word = domainfiltered_movie_dictionary, passes=25)\n",
    "\n",
    "domainfiltered_reviews_ldamodel_highscore.show_topics(num_words=20) #Show the top 8 words for each topic"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Despite the coherence scores saying that filtered model gets the highscore I will use the domainfiltered model because that makes more sense and in practice is easier to use for deciding a topic label. Even with the domainfiltered model it is hard to decide labels for some topics.\n",
    "<br>0- Scary/comedy\n",
    "<br>1- Star wars\n",
    "<br>2- Space action\n",
    "<br>3- Action\n",
    "<br>4- Life"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "To validate your guessed topic label, we could check the probability that the word you chose as a label belongs to each topic. We could do that using the get_term_topics method. For example, the code below checks the probability of the word \"the\" for each topic in the `reviews_ldamodel` model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0.049062002), (3, 0.054476514), (4, 0.04544652)]"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_ldamodel.get_term_topics(\"the\", minimum_probability = 1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do this for your own best model and the labels you just picked. For each of your topic labels, if the probability for the label is the highest for the topic number you wrote down, your guess was probably correct. Did you guess a suitable label for every topic?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0.0016396681), (1, 0.0013902761), (2, 0.0016451329)]"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "domainfiltered_reviews_ldamodel.get_term_topics(\"comedy\", minimum_probability = 1e-3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2, 0.0013919857), (3, 0.0017244702)]"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "domainfiltered_reviews_ldamodel.get_term_topics(\"star\", minimum_probability = 1e-3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "domainfiltered_reviews_ldamodel.get_term_topics(\"space\", minimum_probability = 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 0.0012643057), (2, 0.0018023849), (3, 0.0030721389), (4, 0.0016725643)]"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "domainfiltered_reviews_ldamodel.get_term_topics(\"action\", minimum_probability = 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0.0025653716),\n",
       " (1, 0.0017130619),\n",
       " (2, 0.0029064033),\n",
       " (3, 0.0023465625),\n",
       " (4, 0.0022303644)]"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "domainfiltered_reviews_ldamodel.get_term_topics(\"life\", minimum_probability = 1e-3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>0- validates\n",
    "<br>1- failed\n",
    "<br>2- validates\n",
    "<br>3- validates\n",
    "<br>4- validates"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In my opinion the method of validating is effective, but I assume the statistical occurarnce of the word in the documents depends on the quality of the review too."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a real project, you would also want to validate your topics by examining the reviews that are most strongly associated with that topic. You can see what documents have what topics using the get_document_topics() method. Here we look at the topics for the first document in the model (change the name of the model to yours):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0.0002559949),\n",
       " (1, 0.0002559384),\n",
       " (2, 0.99897516),\n",
       " (3, 0.00025665812),\n",
       " (4, 0.00025624284)]"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "domainfiltered_reviews_ldamodel.get_document_topics(movie_bow_corpus[0], minimum_probability = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or for the first 20 of them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topics for the review cv000_29590.txt: [(2, 0.9974779)]\n",
      "Topics for the review cv001_18431.txt: [(2, 0.9970658)]\n",
      "Topics for the review cv002_15918.txt: [(2, 0.9949002)]\n",
      "Topics for the review cv003_11664.txt: [(3, 0.99819505)]\n",
      "Topics for the review cv004_11636.txt: [(2, 0.9971732)]\n",
      "Topics for the review cv005_29443.txt: [(4, 0.99808145)]\n",
      "Topics for the review cv006_15448.txt: [(3, 0.9972568)]\n",
      "Topics for the review cv007_4968.txt: [(2, 0.99708027)]\n",
      "Topics for the review cv008_29435.txt: [(4, 0.9942103)]\n",
      "Topics for the review cv009_29592.txt: [(1, 0.99637663)]\n",
      "Topics for the review cv010_29198.txt: [(2, 0.99805504)]\n",
      "Topics for the review cv011_12166.txt: [(1, 0.22473091), (2, 0.7729944)]\n",
      "Topics for the review cv012_29576.txt: [(1, 0.9941265)]\n",
      "Topics for the review cv013_10159.txt: [(1, 0.33858004), (4, 0.6573132)]\n",
      "Topics for the review cv014_13924.txt: [(1, 0.34417218), (3, 0.6544162)]\n",
      "Topics for the review cv015_29439.txt: [(2, 0.99653447)]\n",
      "Topics for the review cv016_4659.txt: [(3, 0.99489427)]\n",
      "Topics for the review cv017_22464.txt: [(2, 0.040911417), (3, 0.9573182)]\n",
      "Topics for the review cv018_20137.txt: [(1, 0.96337855), (2, 0.03362045)]\n",
      "Topics for the review cv019_14482.txt: [(2, 0.9455786), (3, 0.052006137)]\n"
     ]
    }
   ],
   "source": [
    "for i, doc_topics in enumerate(domainfiltered_reviews_ldamodel.get_document_topics(domainfiltered_movie_bow_corpus)):\n",
    "    if i >= 20:\n",
    "        break\n",
    "    print(f\"Topics for the review {movie_reviewnames[i]}: {doc_topics}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But this assignment is already long enough so I will not ask you to report on this too!"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
